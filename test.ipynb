{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class EpsilonAnneal(ABC):\n",
    "    @abstractmethod\n",
    "    def anneal(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Constant(EpsilonAnneal):\n",
    "    def __init__(self, start):\n",
    "        self.val = start\n",
    "\n",
    "    def anneal(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class LinearAnneal(EpsilonAnneal):\n",
    "    \"\"\"Linear Annealing Schedule.\n",
    "\n",
    "    Args:\n",
    "        start:      The initial value of epsilon.\n",
    "        end:        The final value of epsilon.\n",
    "        duration:   The number of anneals from start value to end value.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, start: float, end: float, duration: int):\n",
    "        self.val = start\n",
    "        self.min = end\n",
    "        self.duration = duration\n",
    "\n",
    "    def anneal(self):\n",
    "        self.val = max(self.min, self.val - (self.val - self.min) / self.duration)\n",
    "\n",
    "eps = LinearAnneal(1.0, 0.1, 2000000 // 10)\n",
    "eps.min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999424182841602"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps.anneal()\n",
    "eps.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "history_len = 5\n",
    "\n",
    "torch.triu(torch.ones(history_len, history_len), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GELU(approximate='none')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.GELU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiheadAttention(\n",
       "  (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "embed_size = 512\n",
    "num_heads = 8\n",
    "dropout = 0\n",
    "\n",
    "attention = nn.MultiheadAttention(\n",
    "    embed_dim=embed_size,\n",
    "    num_heads=num_heads,\n",
    "    dropout=dropout,\n",
    "    batch_first=True,\n",
    ")\n",
    "\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2407786/254567761.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weight = torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 obs_embedding.observation_embedding.0.weight torch.Size([23, 8])\n",
      "1 obs_embedding.observation_embedding.2.weight torch.Size([256, 48])\n",
      "2 obs_embedding.observation_embedding.2.bias torch.Size([256])\n",
      "3 position_embedding.position_encoding torch.Size([1, 200, 256])\n",
      "4 transformer_layers.0.attn_mask torch.Size([200, 200])\n",
      "5 transformer_layers.0.layernorm1.weight torch.Size([256])\n",
      "6 transformer_layers.0.layernorm1.bias torch.Size([256])\n",
      "7 transformer_layers.0.layernorm2.weight torch.Size([256])\n",
      "8 transformer_layers.0.layernorm2.bias torch.Size([256])\n",
      "9 transformer_layers.0.attention.in_proj_weight torch.Size([768, 256])\n",
      "10 transformer_layers.0.attention.in_proj_bias torch.Size([768])\n",
      "11 transformer_layers.0.attention.out_proj.weight torch.Size([256, 256])\n",
      "12 transformer_layers.0.attention.out_proj.bias torch.Size([256])\n",
      "13 transformer_layers.0.ffn.0.weight torch.Size([1024, 256])\n",
      "14 transformer_layers.0.ffn.0.bias torch.Size([1024])\n",
      "15 transformer_layers.0.ffn.2.weight torch.Size([256, 1024])\n",
      "16 transformer_layers.0.ffn.2.bias torch.Size([256])\n",
      "17 transformer_layers.1.attn_mask torch.Size([200, 200])\n",
      "18 transformer_layers.1.layernorm1.weight torch.Size([256])\n",
      "19 transformer_layers.1.layernorm1.bias torch.Size([256])\n",
      "20 transformer_layers.1.layernorm2.weight torch.Size([256])\n",
      "21 transformer_layers.1.layernorm2.bias torch.Size([256])\n",
      "22 transformer_layers.1.attention.in_proj_weight torch.Size([768, 256])\n",
      "23 transformer_layers.1.attention.in_proj_bias torch.Size([768])\n",
      "24 transformer_layers.1.attention.out_proj.weight torch.Size([256, 256])\n",
      "25 transformer_layers.1.attention.out_proj.bias torch.Size([256])\n",
      "26 transformer_layers.1.ffn.0.weight torch.Size([1024, 256])\n",
      "27 transformer_layers.1.ffn.0.bias torch.Size([1024])\n",
      "28 transformer_layers.1.ffn.2.weight torch.Size([256, 1024])\n",
      "29 transformer_layers.1.ffn.2.bias torch.Size([256])\n",
      "30 transformer_layers.2.attn_mask torch.Size([200, 200])\n",
      "31 transformer_layers.2.layernorm1.weight torch.Size([256])\n",
      "32 transformer_layers.2.layernorm1.bias torch.Size([256])\n",
      "33 transformer_layers.2.layernorm2.weight torch.Size([256])\n",
      "34 transformer_layers.2.layernorm2.bias torch.Size([256])\n",
      "35 transformer_layers.2.attention.in_proj_weight torch.Size([768, 256])\n",
      "36 transformer_layers.2.attention.in_proj_bias torch.Size([768])\n",
      "37 transformer_layers.2.attention.out_proj.weight torch.Size([256, 256])\n",
      "38 transformer_layers.2.attention.out_proj.bias torch.Size([256])\n",
      "39 transformer_layers.2.ffn.0.weight torch.Size([1024, 256])\n",
      "40 transformer_layers.2.ffn.0.bias torch.Size([1024])\n",
      "41 transformer_layers.2.ffn.2.weight torch.Size([256, 1024])\n",
      "42 transformer_layers.2.ffn.2.bias torch.Size([256])\n",
      "43 transformer_layers.3.attn_mask torch.Size([200, 200])\n",
      "44 transformer_layers.3.layernorm1.weight torch.Size([256])\n",
      "45 transformer_layers.3.layernorm1.bias torch.Size([256])\n",
      "46 transformer_layers.3.layernorm2.weight torch.Size([256])\n",
      "47 transformer_layers.3.layernorm2.bias torch.Size([256])\n",
      "48 transformer_layers.3.attention.in_proj_weight torch.Size([768, 256])\n",
      "49 transformer_layers.3.attention.in_proj_bias torch.Size([768])\n",
      "50 transformer_layers.3.attention.out_proj.weight torch.Size([256, 256])\n",
      "51 transformer_layers.3.attention.out_proj.bias torch.Size([256])\n",
      "52 transformer_layers.3.ffn.0.weight torch.Size([1024, 256])\n",
      "53 transformer_layers.3.ffn.0.bias torch.Size([1024])\n",
      "54 transformer_layers.3.ffn.2.weight torch.Size([256, 1024])\n",
      "55 transformer_layers.3.ffn.2.bias torch.Size([256])\n",
      "56 transformer_layers.4.attn_mask torch.Size([200, 200])\n",
      "57 transformer_layers.4.layernorm1.weight torch.Size([256])\n",
      "58 transformer_layers.4.layernorm1.bias torch.Size([256])\n",
      "59 transformer_layers.4.layernorm2.weight torch.Size([256])\n",
      "60 transformer_layers.4.layernorm2.bias torch.Size([256])\n",
      "61 transformer_layers.4.attention.in_proj_weight torch.Size([768, 256])\n",
      "62 transformer_layers.4.attention.in_proj_bias torch.Size([768])\n",
      "63 transformer_layers.4.attention.out_proj.weight torch.Size([256, 256])\n",
      "64 transformer_layers.4.attention.out_proj.bias torch.Size([256])\n",
      "65 transformer_layers.4.ffn.0.weight torch.Size([1024, 256])\n",
      "66 transformer_layers.4.ffn.0.bias torch.Size([1024])\n",
      "67 transformer_layers.4.ffn.2.weight torch.Size([256, 1024])\n",
      "68 transformer_layers.4.ffn.2.bias torch.Size([256])\n",
      "69 transformer_layers.5.attn_mask torch.Size([200, 200])\n",
      "70 transformer_layers.5.layernorm1.weight torch.Size([256])\n",
      "71 transformer_layers.5.layernorm1.bias torch.Size([256])\n",
      "72 transformer_layers.5.layernorm2.weight torch.Size([256])\n",
      "73 transformer_layers.5.layernorm2.bias torch.Size([256])\n",
      "74 transformer_layers.5.attention.in_proj_weight torch.Size([768, 256])\n",
      "75 transformer_layers.5.attention.in_proj_bias torch.Size([768])\n",
      "76 transformer_layers.5.attention.out_proj.weight torch.Size([256, 256])\n",
      "77 transformer_layers.5.attention.out_proj.bias torch.Size([256])\n",
      "78 transformer_layers.5.ffn.0.weight torch.Size([1024, 256])\n",
      "79 transformer_layers.5.ffn.0.bias torch.Size([1024])\n",
      "80 transformer_layers.5.ffn.2.weight torch.Size([256, 1024])\n",
      "81 transformer_layers.5.ffn.2.bias torch.Size([256])\n",
      "82 transformer_layers.6.attn_mask torch.Size([200, 200])\n",
      "83 transformer_layers.6.layernorm1.weight torch.Size([256])\n",
      "84 transformer_layers.6.layernorm1.bias torch.Size([256])\n",
      "85 transformer_layers.6.layernorm2.weight torch.Size([256])\n",
      "86 transformer_layers.6.layernorm2.bias torch.Size([256])\n",
      "87 transformer_layers.6.attention.in_proj_weight torch.Size([768, 256])\n",
      "88 transformer_layers.6.attention.in_proj_bias torch.Size([768])\n",
      "89 transformer_layers.6.attention.out_proj.weight torch.Size([256, 256])\n",
      "90 transformer_layers.6.attention.out_proj.bias torch.Size([256])\n",
      "91 transformer_layers.6.ffn.0.weight torch.Size([1024, 256])\n",
      "92 transformer_layers.6.ffn.0.bias torch.Size([1024])\n",
      "93 transformer_layers.6.ffn.2.weight torch.Size([256, 1024])\n",
      "94 transformer_layers.6.ffn.2.bias torch.Size([256])\n",
      "95 transformer_layers.7.attn_mask torch.Size([200, 200])\n",
      "96 transformer_layers.7.layernorm1.weight torch.Size([256])\n",
      "97 transformer_layers.7.layernorm1.bias torch.Size([256])\n",
      "98 transformer_layers.7.layernorm2.weight torch.Size([256])\n",
      "99 transformer_layers.7.layernorm2.bias torch.Size([256])\n",
      "100 transformer_layers.7.attention.in_proj_weight torch.Size([768, 256])\n",
      "101 transformer_layers.7.attention.in_proj_bias torch.Size([768])\n",
      "102 transformer_layers.7.attention.out_proj.weight torch.Size([256, 256])\n",
      "103 transformer_layers.7.attention.out_proj.bias torch.Size([256])\n",
      "104 transformer_layers.7.ffn.0.weight torch.Size([1024, 256])\n",
      "105 transformer_layers.7.ffn.0.bias torch.Size([1024])\n",
      "106 transformer_layers.7.ffn.2.weight torch.Size([256, 1024])\n",
      "107 transformer_layers.7.ffn.2.bias torch.Size([256])\n",
      "108 transformer_layers.8.attn_mask torch.Size([200, 200])\n",
      "109 transformer_layers.8.layernorm1.weight torch.Size([256])\n",
      "110 transformer_layers.8.layernorm1.bias torch.Size([256])\n",
      "111 transformer_layers.8.layernorm2.weight torch.Size([256])\n",
      "112 transformer_layers.8.layernorm2.bias torch.Size([256])\n",
      "113 transformer_layers.8.attention.in_proj_weight torch.Size([768, 256])\n",
      "114 transformer_layers.8.attention.in_proj_bias torch.Size([768])\n",
      "115 transformer_layers.8.attention.out_proj.weight torch.Size([256, 256])\n",
      "116 transformer_layers.8.attention.out_proj.bias torch.Size([256])\n",
      "117 transformer_layers.8.ffn.0.weight torch.Size([1024, 256])\n",
      "118 transformer_layers.8.ffn.0.bias torch.Size([1024])\n",
      "119 transformer_layers.8.ffn.2.weight torch.Size([256, 1024])\n",
      "120 transformer_layers.8.ffn.2.bias torch.Size([256])\n",
      "121 transformer_layers.9.attn_mask torch.Size([200, 200])\n",
      "122 transformer_layers.9.layernorm1.weight torch.Size([256])\n",
      "123 transformer_layers.9.layernorm1.bias torch.Size([256])\n",
      "124 transformer_layers.9.layernorm2.weight torch.Size([256])\n",
      "125 transformer_layers.9.layernorm2.bias torch.Size([256])\n",
      "126 transformer_layers.9.attention.in_proj_weight torch.Size([768, 256])\n",
      "127 transformer_layers.9.attention.in_proj_bias torch.Size([768])\n",
      "128 transformer_layers.9.attention.out_proj.weight torch.Size([256, 256])\n",
      "129 transformer_layers.9.attention.out_proj.bias torch.Size([256])\n",
      "130 transformer_layers.9.ffn.0.weight torch.Size([1024, 256])\n",
      "131 transformer_layers.9.ffn.0.bias torch.Size([1024])\n",
      "132 transformer_layers.9.ffn.2.weight torch.Size([256, 1024])\n",
      "133 transformer_layers.9.ffn.2.bias torch.Size([256])\n",
      "134 transformer_layers.10.attn_mask torch.Size([200, 200])\n",
      "135 transformer_layers.10.layernorm1.weight torch.Size([256])\n",
      "136 transformer_layers.10.layernorm1.bias torch.Size([256])\n",
      "137 transformer_layers.10.layernorm2.weight torch.Size([256])\n",
      "138 transformer_layers.10.layernorm2.bias torch.Size([256])\n",
      "139 transformer_layers.10.attention.in_proj_weight torch.Size([768, 256])\n",
      "140 transformer_layers.10.attention.in_proj_bias torch.Size([768])\n",
      "141 transformer_layers.10.attention.out_proj.weight torch.Size([256, 256])\n",
      "142 transformer_layers.10.attention.out_proj.bias torch.Size([256])\n",
      "143 transformer_layers.10.ffn.0.weight torch.Size([1024, 256])\n",
      "144 transformer_layers.10.ffn.0.bias torch.Size([1024])\n",
      "145 transformer_layers.10.ffn.2.weight torch.Size([256, 1024])\n",
      "146 transformer_layers.10.ffn.2.bias torch.Size([256])\n",
      "147 transformer_layers.11.attn_mask torch.Size([200, 200])\n",
      "148 transformer_layers.11.layernorm1.weight torch.Size([256])\n",
      "149 transformer_layers.11.layernorm1.bias torch.Size([256])\n",
      "150 transformer_layers.11.layernorm2.weight torch.Size([256])\n",
      "151 transformer_layers.11.layernorm2.bias torch.Size([256])\n",
      "152 transformer_layers.11.attention.in_proj_weight torch.Size([768, 256])\n",
      "153 transformer_layers.11.attention.in_proj_bias torch.Size([768])\n",
      "154 transformer_layers.11.attention.out_proj.weight torch.Size([256, 256])\n",
      "155 transformer_layers.11.attention.out_proj.bias torch.Size([256])\n",
      "156 transformer_layers.11.ffn.0.weight torch.Size([1024, 256])\n",
      "157 transformer_layers.11.ffn.0.bias torch.Size([1024])\n",
      "158 transformer_layers.11.ffn.2.weight torch.Size([256, 1024])\n",
      "159 transformer_layers.11.ffn.2.bias torch.Size([256])\n",
      "160 transformer_layers.12.attn_mask torch.Size([200, 200])\n",
      "161 transformer_layers.12.layernorm1.weight torch.Size([256])\n",
      "162 transformer_layers.12.layernorm1.bias torch.Size([256])\n",
      "163 transformer_layers.12.layernorm2.weight torch.Size([256])\n",
      "164 transformer_layers.12.layernorm2.bias torch.Size([256])\n",
      "165 transformer_layers.12.attention.in_proj_weight torch.Size([768, 256])\n",
      "166 transformer_layers.12.attention.in_proj_bias torch.Size([768])\n",
      "167 transformer_layers.12.attention.out_proj.weight torch.Size([256, 256])\n",
      "168 transformer_layers.12.attention.out_proj.bias torch.Size([256])\n",
      "169 transformer_layers.12.ffn.0.weight torch.Size([1024, 256])\n",
      "170 transformer_layers.12.ffn.0.bias torch.Size([1024])\n",
      "171 transformer_layers.12.ffn.2.weight torch.Size([256, 1024])\n",
      "172 transformer_layers.12.ffn.2.bias torch.Size([256])\n",
      "173 transformer_layers.13.attn_mask torch.Size([200, 200])\n",
      "174 transformer_layers.13.layernorm1.weight torch.Size([256])\n",
      "175 transformer_layers.13.layernorm1.bias torch.Size([256])\n",
      "176 transformer_layers.13.layernorm2.weight torch.Size([256])\n",
      "177 transformer_layers.13.layernorm2.bias torch.Size([256])\n",
      "178 transformer_layers.13.attention.in_proj_weight torch.Size([768, 256])\n",
      "179 transformer_layers.13.attention.in_proj_bias torch.Size([768])\n",
      "180 transformer_layers.13.attention.out_proj.weight torch.Size([256, 256])\n",
      "181 transformer_layers.13.attention.out_proj.bias torch.Size([256])\n",
      "182 transformer_layers.13.ffn.0.weight torch.Size([1024, 256])\n",
      "183 transformer_layers.13.ffn.0.bias torch.Size([1024])\n",
      "184 transformer_layers.13.ffn.2.weight torch.Size([256, 1024])\n",
      "185 transformer_layers.13.ffn.2.bias torch.Size([256])\n",
      "186 transformer_layers.14.attn_mask torch.Size([200, 200])\n",
      "187 transformer_layers.14.layernorm1.weight torch.Size([256])\n",
      "188 transformer_layers.14.layernorm1.bias torch.Size([256])\n",
      "189 transformer_layers.14.layernorm2.weight torch.Size([256])\n",
      "190 transformer_layers.14.layernorm2.bias torch.Size([256])\n",
      "191 transformer_layers.14.attention.in_proj_weight torch.Size([768, 256])\n",
      "192 transformer_layers.14.attention.in_proj_bias torch.Size([768])\n",
      "193 transformer_layers.14.attention.out_proj.weight torch.Size([256, 256])\n",
      "194 transformer_layers.14.attention.out_proj.bias torch.Size([256])\n",
      "195 transformer_layers.14.ffn.0.weight torch.Size([1024, 256])\n",
      "196 transformer_layers.14.ffn.0.bias torch.Size([1024])\n",
      "197 transformer_layers.14.ffn.2.weight torch.Size([256, 1024])\n",
      "198 transformer_layers.14.ffn.2.bias torch.Size([256])\n",
      "199 transformer_layers.15.attn_mask torch.Size([200, 200])\n",
      "200 transformer_layers.15.layernorm1.weight torch.Size([256])\n",
      "201 transformer_layers.15.layernorm1.bias torch.Size([256])\n",
      "202 transformer_layers.15.layernorm2.weight torch.Size([256])\n",
      "203 transformer_layers.15.layernorm2.bias torch.Size([256])\n",
      "204 transformer_layers.15.attention.in_proj_weight torch.Size([768, 256])\n",
      "205 transformer_layers.15.attention.in_proj_bias torch.Size([768])\n",
      "206 transformer_layers.15.attention.out_proj.weight torch.Size([256, 256])\n",
      "207 transformer_layers.15.attention.out_proj.bias torch.Size([256])\n",
      "208 transformer_layers.15.ffn.0.weight torch.Size([1024, 256])\n",
      "209 transformer_layers.15.ffn.0.bias torch.Size([1024])\n",
      "210 transformer_layers.15.ffn.2.weight torch.Size([256, 1024])\n",
      "211 transformer_layers.15.ffn.2.bias torch.Size([256])\n",
      "212 ffn.0.weight torch.Size([256, 256])\n",
      "213 ffn.0.bias torch.Size([256])\n",
      "214 ffn.2.weight torch.Size([6, 256])\n",
      "215 ffn.2.bias torch.Size([6])\n",
      "\n",
      "13407422\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# path = '/data/kimgh/workspace/DTQN/policies/DTQN-test-layers_8/gv_memory.9x9.yaml/model=DTQN_envs=gv_memory.9x9.yaml_obs_embed=8_a_embed=0_in_embed=128_context=100_heads=8_layers=8_batch=32_gate=res_identity=False_history=100_pos=learned_bag=0_seed=1'\n",
    "# path = '/data/kimgh/workspace/DTQN/policies/DTQN-test-steps_4M/gv_memory.5x5.yaml/model=DTQN_envs=gv_memory.5x5.yaml_obs_embed=8_a_embed=0_in_embed=128_context=50_heads=8_layers=2_batch=32_gate=res_identity=False_history=50_pos=learned_bag=0_seed=1'\n",
    "path = '/data/kimgh/workspace/DTQN/policies/DTQN-test-custom/gv_memory.21x21.yaml/model=DTQN_envs=gv_memory.21x21.yaml_obs_embed=8_a_embed=0_in_embed=256_context=200_heads=16_layers=16_batch=64_gate=res_identity=False_history=200_pos=learned_bag=0_seed=1'\n",
    "\n",
    "weight = torch.load(path)\n",
    "\n",
    "for i, (k, v) in enumerate(weight.items()):\n",
    "    print(i, k, v.shape)\n",
    "    \n",
    "# 총 가중치 개수 계산\n",
    "total_params = sum(param.numel() for param in weight.values())\n",
    "print('')\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8413, 1.9545, 2.9959])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "F.gelu(torch.tensor([1.0, 2.0, 3.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8413, 1.9545, 2.9959])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "F.gelu(torch.tensor([1.0, 2.0, 3.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniforge3/envs/dtqn/lib/python3.11/site-packages/gym_gridverse/gym.py:9: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: ``gym_pomdps`` is not installed. This means you cannot run an experiment with the HeavenHell or Hallway domain. \n",
      "WARNING: ``mini_hack`` is not installed. This means you cannot run an experiment with any of the MH- domains.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers.time_limit import TimeLimit\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "\n",
    "try:\n",
    "    from gym_gridverse.gym import GymEnvironment\n",
    "    from gym_gridverse.envs.yaml.factory import factory_env_from_yaml\n",
    "    from gym_gridverse.outer_env import OuterEnv\n",
    "    from gym_gridverse.representations.observation_representations import (\n",
    "        make_observation_representation,\n",
    "    )\n",
    "    from gym_gridverse.representations.state_representations import (\n",
    "        make_state_representation,\n",
    "    )\n",
    "except ImportError:\n",
    "    print(\n",
    "        f\"WARNING: ``gym_gridverse`` is not installed. This means you cannot run an experiment with the `gv_*` domains.\"\n",
    "    )\n",
    "    GymEnvironment = None\n",
    "from envs.gv_wrapper import GridVerseWrapper\n",
    "import os\n",
    "from enum import Enum\n",
    "from typing import Tuple\n",
    "\n",
    "from utils.random import RNG\n",
    "\n",
    "\n",
    "def make_env(id_or_path: str) -> GymEnvironment:\n",
    "    \"\"\"Makes a GV gym environment.\"\"\"\n",
    "    try:\n",
    "        print(\"Loading using gym.make\")\n",
    "        env = gym.make(id_or_path)\n",
    "\n",
    "    except gym.error.Error:\n",
    "        print(f\"Environment with id {id_or_path} not found.\")\n",
    "        print(\"Loading using YAML\")\n",
    "        inner_env = factory_env_from_yaml(\n",
    "            os.path.join(os.getcwd(), \"envs\", \"gridverse\", id_or_path)\n",
    "        )\n",
    "        state_representation = make_state_representation(\n",
    "            \"default\", inner_env.state_space\n",
    "        )\n",
    "        observation_representation = make_observation_representation(\n",
    "            \"default\", inner_env.observation_space\n",
    "        )\n",
    "        outer_env = OuterEnv(\n",
    "            inner_env,\n",
    "            state_representation=state_representation,\n",
    "            observation_representation=observation_representation,\n",
    "        )\n",
    "        env = GymEnvironment(outer_env)\n",
    "        env = TimeLimit(GridVerseWrapper(env), max_episode_steps=250)\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "class ObsType(Enum):\n",
    "    DISCRETE = 0\n",
    "    CONTINUOUS = 1\n",
    "    IMAGE = 2\n",
    "\n",
    "\n",
    "def get_env_obs_type(env: gym.Env) -> int:\n",
    "    obs_space = env.observation_space\n",
    "    sample_obs = env.reset()\n",
    "    # Check for image first\n",
    "    if (\n",
    "        (isinstance(sample_obs, np.ndarray) and len(sample_obs.shape) == 3)\n",
    "        and isinstance(obs_space, spaces.Box)\n",
    "        and np.all(obs_space.low == 0)\n",
    "        and np.all(obs_space.high == 255)\n",
    "    ):\n",
    "        return ObsType.IMAGE\n",
    "    elif isinstance(\n",
    "        obs_space, (spaces.Discrete, spaces.MultiDiscrete, spaces.MultiBinary)\n",
    "    ):\n",
    "        return ObsType.DISCRETE\n",
    "    else:\n",
    "        return ObsType.CONTINUOUS\n",
    "\n",
    "\n",
    "def get_env_obs_length(env: gym.Env) -> int:\n",
    "    \"\"\"Gets the length of the observations in an environment\"\"\"\n",
    "    if get_env_obs_type(env) == ObsType.IMAGE:\n",
    "        return env.reset().shape\n",
    "    elif isinstance(env.observation_space, gym.spaces.Discrete):\n",
    "        return 1\n",
    "    elif isinstance(env.observation_space, (gym.spaces.MultiDiscrete, gym.spaces.Box)):\n",
    "        if len(env.observation_space.shape) != 1:\n",
    "            raise NotImplementedError(f\"We do not yet support 2D observation spaces\")\n",
    "        return env.observation_space.shape[0]\n",
    "    elif isinstance(env.observation_space, spaces.MultiBinary):\n",
    "        return env.observation_space.n\n",
    "    else:\n",
    "        raise NotImplementedError(f\"We do not yet support {env.observation_space}\")\n",
    "\n",
    "\n",
    "def get_env_obs_mask(env: gym.Env) -> Union[int, np.ndarray]:\n",
    "    \"\"\"Gets the number of observations possible (for discrete case).\n",
    "    For continuous case, please edit the -5 to something lower than\n",
    "    lowest possible observation (while still being finite) so the\n",
    "    network knows it is padding.\n",
    "    \"\"\"\n",
    "    # Check image first\n",
    "    if get_env_obs_type(env) == ObsType.IMAGE:\n",
    "        return 0\n",
    "    if isinstance(env.observation_space, gym.spaces.Discrete):\n",
    "        return env.observation_space.n\n",
    "    elif isinstance(env.observation_space, gym.spaces.MultiDiscrete):\n",
    "        return max(env.observation_space.nvec) + 1\n",
    "    elif isinstance(env.observation_space, gym.spaces.Box):\n",
    "        # If you would like to use DTQN with a continuous action space, make sure this value is\n",
    "        #       below the minimum possible observation. Otherwise it will appear as a real observation\n",
    "        #       to the network which may cause issues. In our case, Car Flag has min of -1 so this is\n",
    "        #       fine.\n",
    "        return -5\n",
    "    else:\n",
    "        raise NotImplementedError(f\"We do not yet support {env.observation_space}\")\n",
    "\n",
    "\n",
    "def get_env_max_steps(env: gym.Env) -> Union[int, None]:\n",
    "    \"\"\"Gets the maximum steps allowed in an episode before auto-terminating\"\"\"\n",
    "    try:\n",
    "        return env._max_episode_steps\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            return env.max_episode_steps\n",
    "        except AttributeError:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading using gym.make\n",
      "Environment with id gv_memory.5x5.yaml not found.\n",
      "Loading using YAML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniforge3/envs/dtqn/lib/python3.11/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = make_env('gv_memory.5x5.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('agent': Box([-1. -1.  0.  0.  0.  0.], 1.0, (6,), float64), 'agent_id_grid': Box(0, 1, (5, 5), int64), 'grid': Box(0, [[[10  1  4]\n",
       "  [10  1  4]\n",
       "  [10  1  4]\n",
       "  [10  1  4]\n",
       "  [10  1  4]]\n",
       "\n",
       " [[10  1  4]\n",
       "  [10  1  4]\n",
       "  [10  1  4]\n",
       "  [10  1  4]\n",
       "  [10  1  4]]\n",
       "\n",
       " [[10  1  4]\n",
       "  [10  1  4]\n",
       "  [10  1  4]\n",
       "  [10  1  4]\n",
       "  [10  1  4]]\n",
       "\n",
       " [[10  1  4]\n",
       "  [10  1  4]\n",
       "  [10  1  4]\n",
       "  [10  1  4]\n",
       "  [10  1  4]]\n",
       "\n",
       " [[10  1  4]\n",
       "  [10  1  4]\n",
       "  [10  1  4]\n",
       "  [10  1  4]\n",
       "  [10  1  4]]], (5, 5, 3), int64), 'item': Box(0, [10  1  4], (3,), int64))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method name: __class_getitem__, Method: <bound method Generic.__class_getitem__ of <class 'gym.wrappers.time_limit.TimeLimit'>>\n",
      "Method name: __enter__, Method: <bound method Env.__enter__ of <TimeLimit<GridVerseWrapper<GymEnvironment instance>>>>\n",
      "Method name: __exit__, Method: <bound method Env.__exit__ of <TimeLimit<GridVerseWrapper<GymEnvironment instance>>>>\n",
      "Method name: __getattr__, Method: <bound method Wrapper.__getattr__ of <TimeLimit<GridVerseWrapper<GymEnvironment instance>>>>\n",
      "Method name: __init__, Method: <bound method TimeLimit.__init__ of <TimeLimit<GridVerseWrapper<GymEnvironment instance>>>>\n",
      "Method name: __init_subclass__, Method: <bound method Env.__init_subclass__ of <class 'gym.wrappers.time_limit.TimeLimit'>>\n",
      "Method name: __repr__, Method: <bound method Wrapper.__repr__ of <TimeLimit<GridVerseWrapper<GymEnvironment instance>>>>\n",
      "Method name: __str__, Method: <bound method Wrapper.__str__ of <TimeLimit<GridVerseWrapper<GymEnvironment instance>>>>\n",
      "Method name: class_name, Method: <bound method Wrapper.class_name of <class 'gym.wrappers.time_limit.TimeLimit'>>\n",
      "Method name: close, Method: <bound method Wrapper.close of <TimeLimit<GridVerseWrapper<GymEnvironment instance>>>>\n",
      "Method name: render, Method: <bound method _deprecate_mode.<locals>.render of <TimeLimit<GridVerseWrapper<GymEnvironment instance>>>>\n",
      "Method name: reset, Method: <bound method TimeLimit.reset of <TimeLimit<GridVerseWrapper<GymEnvironment instance>>>>\n",
      "Method name: seed, Method: <bound method Wrapper.seed of <TimeLimit<GridVerseWrapper<GymEnvironment instance>>>>\n",
      "Method name: step, Method: <bound method TimeLimit.step of <TimeLimit<GridVerseWrapper<GymEnvironment instance>>>>\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "obj = env  # 실제 객체 할당\n",
    "methods = inspect.getmembers(obj, predicate=inspect.ismethod)\n",
    "for name, method in methods:\n",
    "    print(f\"Method name: {name}, Method: {method}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TimeLimit in module gym.wrappers.time_limit object:\n",
      "\n",
      "class TimeLimit(gym.core.Wrapper)\n",
      " |  TimeLimit(env: gym.core.Env, max_episode_steps: Optional[int] = None, new_step_api: bool = False)\n",
      " |  \n",
      " |  This wrapper will issue a `truncated` signal if a maximum number of timesteps is exceeded.\n",
      " |  \n",
      " |  If a truncation is not defined inside the environment itself, this is the only place that the truncation signal is issued.\n",
      " |  Critically, this is different from the `terminated` signal that originates from the underlying environment as part of the MDP.\n",
      " |  \n",
      " |  (deprecated)\n",
      " |  This information is passed through ``info`` that is returned when `done`-signal was issued.\n",
      " |  The done-signal originates from the time limit (i.e. it signifies a *truncation*) if and only if\n",
      " |  the key `\"TimeLimit.truncated\"` exists in ``info`` and the corresponding value is ``True``. This will be removed in favour\n",
      " |  of only issuing a `truncated` signal in future versions.\n",
      " |  \n",
      " |  Example:\n",
      " |     >>> from gym.envs.classic_control import CartPoleEnv\n",
      " |     >>> from gym.wrappers import TimeLimit\n",
      " |     >>> env = CartPoleEnv()\n",
      " |     >>> env = TimeLimit(env, max_episode_steps=1000)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TimeLimit\n",
      " |      gym.core.Wrapper\n",
      " |      gym.core.Env\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, env: gym.core.Env, max_episode_steps: Optional[int] = None, new_step_api: bool = False)\n",
      " |      Initializes the :class:`TimeLimit` wrapper with an environment and the number of steps after which truncation will occur.\n",
      " |      \n",
      " |      Args:\n",
      " |          env: The environment to apply the wrapper\n",
      " |          max_episode_steps: An optional max episode steps (if ``None``, ``env.spec.max_episode_steps`` is used)\n",
      " |          new_step_api (bool): Whether the wrapper's step method outputs two booleans (new API) or one boolean (old API)\n",
      " |  \n",
      " |  reset(self, **kwargs)\n",
      " |      Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: The kwargs to reset the environment with\n",
      " |      \n",
      " |      Returns:\n",
      " |          The reset environment\n",
      " |  \n",
      " |  step(self, action)\n",
      " |      Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\n",
      " |      \n",
      " |      Args:\n",
      " |          action: The environment step action\n",
      " |      \n",
      " |      Returns:\n",
      " |          The environment step ``(observation, reward, done, info)`` with \"TimeLimit.truncated\"=True\n",
      " |          when truncated (the number of steps elapsed >= max episode steps) or\n",
      " |          \"TimeLimit.truncated\"=False if the environment terminated\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gym.core.Wrapper:\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |      Returns an attribute with ``name``, unless ``name`` starts with an underscore.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Returns the string representation of the wrapper.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Returns the wrapper name and the unwrapped environment string.\n",
      " |  \n",
      " |  close(self)\n",
      " |      Closes the environment.\n",
      " |  \n",
      " |  render(self: object, *args: Tuple[Any], **kwargs: Dict[str, Any]) -> Union[~RenderFrame, List[~RenderFrame], NoneType]\n",
      " |  \n",
      " |  seed(self, seed=None)\n",
      " |      Seeds the environment.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gym.core.Wrapper:\n",
      " |  \n",
      " |  class_name() from builtins.type\n",
      " |      Returns the class name of the wrapper.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from gym.core.Wrapper:\n",
      " |  \n",
      " |  render_mode\n",
      " |      Returns the environment render_mode.\n",
      " |  \n",
      " |  spec\n",
      " |      Returns the environment specification.\n",
      " |  \n",
      " |  unwrapped\n",
      " |      Returns the base environment of the wrapper.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gym.core.Wrapper:\n",
      " |  \n",
      " |  action_space\n",
      " |      Returns the action space of the environment.\n",
      " |  \n",
      " |  metadata\n",
      " |      Returns the environment metadata.\n",
      " |  \n",
      " |  np_random\n",
      " |      Returns the environment np_random.\n",
      " |  \n",
      " |  observation_space\n",
      " |      Returns the observation space of the environment.\n",
      " |  \n",
      " |  reward_range\n",
      " |      Return the reward range of the environment.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from gym.core.Wrapper:\n",
      " |  \n",
      " |  __orig_bases__ = (gym.core.Env[~ObsType, ~ActType],)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gym.core.Env:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __exit__(self, *args)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gym.core.Env:\n",
      " |  \n",
      " |  __init_subclass__() -> None from builtins.type\n",
      " |      Hook used for wrapping render method.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gym.core.Env:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |      Parameterizes a generic class.\n",
      " |      \n",
      " |      At least, parameterizing a generic class is the *main* thing this method\n",
      " |      does. For example, for some generic class `Foo`, this is called when we\n",
      " |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |      \n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
